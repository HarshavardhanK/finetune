model:
  name: "llama3"
  huggingface_id: "meta-llama/Meta-Llama-3.2-7B"
  variant: "7B"  
  quantization: "4bit"  #Options: 4bit, 8bit, none
  peft_method: "lora"   #Options: lora, qlora, adapter

training:

  batch_size: 8
  gradient_accumulation_steps: 4
  learning_rate: 2e-4
  
  warmup_steps: 100
  max_steps: 1000
  save_steps: 100
  eval_steps: 100

  lora_config:
    r: 8
    alpha: 32
    dropout: 0.1
    target_modules: ["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]

hardware:

  instance_type: "trn1.32xlarge"
  num_nodes: 1
  num_gpus_per_node: 8
  distributed_strategy: "fsdp"  #options: fsdp, deepspeed, megatron 